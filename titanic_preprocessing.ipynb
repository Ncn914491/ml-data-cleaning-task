{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Internship Task: Data Cleaning & Preprocessing\n",
    "\n",
    "This notebook demonstrates the complete data preprocessing workflow for the Titanic dataset, including:\n",
    "1. Data exploration and basic information\n",
    "2. Handling missing values\n",
    "3. Encoding categorical features\n",
    "4. Normalizing/standardizing numerical features\n",
    "5. Visualizing and removing outliers\n",
    "\n",
    "The notebook also includes answers to common interview questions related to data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy import stats\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check data types and non-null counts\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the distribution of the target variable (Survived)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Survived', data=df)\n",
    "plt.title('Survival Distribution')\n",
    "plt.xlabel('Survived (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the distribution of categorical features\n",
    "plt.figure(figsize=(15, 10))\n",
    "categorical_features = ['Pclass', 'Sex']\n",
    "for i, feature in enumerate(categorical_features, 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    sns.countplot(x=feature, data=df, hue='Survived')\n",
    "    plt.title(f'{feature} Distribution by Survival')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the distribution of numerical features\n",
    "plt.figure(figsize=(15, 10))\n",
    "numerical_features = ['Age', 'Fare', 'Siblings/Spouses Aboard', 'Parents/Children Aboard']\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.histplot(df[feature], kde=True)\n",
    "    plt.title(f'{feature} Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling Missing Values\n",
    "\n",
    "In this dataset, we don't have any missing values as confirmed by the null check above. However, in real-world scenarios, we would typically handle missing values using techniques like:\n",
    "\n",
    "1. **Deletion**: Remove rows or columns with missing values\n",
    "2. **Imputation**: Fill missing values with mean, median, mode, or predicted values\n",
    "3. **Advanced methods**: Use algorithms like KNN or regression for imputation\n",
    "\n",
    "Below is an example of how we would handle missing values if they existed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a copy of the dataframe for demonstration\n",
    "df_with_missing = df.copy()\n",
    "\n",
    "# Artificially introduce some missing values for demonstration\n",
    "np.random.seed(42)\n",
    "mask = np.random.random(df_with_missing.shape) < 0.05  # 5% of data will be missing\n",
    "df_with_missing = df_with_missing.mask(mask)\n",
    "\n",
    "# Check the artificially introduced missing values\n",
    "print(\"Artificially introduced missing values:\")\n",
    "print(df_with_missing.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Handle missing values for numerical features using mean imputation\n",
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "df_with_missing[numerical_features] = numerical_imputer.fit_transform(df_with_missing[numerical_features])\n",
    "\n",
    "# Handle missing values for categorical features using most frequent value\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_with_missing[categorical_features] = categorical_imputer.fit_transform(df_with_missing[categorical_features])\n",
    "\n",
    "# Check if missing values are handled\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_with_missing.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoding Categorical Features\n",
    "\n",
    "In this dataset, we need to encode the 'Sex' column which is categorical. We'll demonstrate both label encoding and one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a copy of the dataframe for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# 1. Label Encoding for Sex column\n",
    "print(\"Label Encoding for 'Sex' column\")\n",
    "label_encoder = LabelEncoder()\n",
    "df_processed['Sex_Label'] = label_encoder.fit_transform(df_processed['Sex'])\n",
    "print(\"Label encoding mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "print(\"\\nFirst 5 rows after label encoding:\")\n",
    "df_processed[['Sex', 'Sex_Label']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2. One-Hot Encoding for Sex column\n",
    "print(\"One-Hot Encoding for 'Sex' column\")\n",
    "# Using pandas get_dummies for one-hot encoding\n",
    "df_onehot = pd.get_dummies(df_processed['Sex'], prefix='Sex')\n",
    "df_processed = pd.concat([df_processed, df_onehot], axis=1)\n",
    "print(\"\\nFirst 5 rows after one-hot encoding:\")\n",
    "df_processed[['Sex', 'Sex_male', 'Sex_female']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the distribution of encoded features\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='Sex_Label', data=df_processed, hue='Survived')\n",
    "plt.title('Sex (Label Encoded) vs Survival')\n",
    "plt.xlabel('Sex (0=female, 1=male)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Calculate survival rate by sex\n",
    "survival_by_sex = df_processed.groupby('Sex')['Survived'].mean().reset_index()\n",
    "sns.barplot(x='Sex', y='Survived', data=survival_by_sex)\n",
    "plt.title('Survival Rate by Sex')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalizing/Standardizing Numerical Features\n",
    "\n",
    "We'll demonstrate both standardization (z-score normalization) and min-max normalization on the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a DataFrame to store original and scaled values for comparison\n",
    "scaling_comparison = pd.DataFrame()\n",
    "for feature in numerical_features:\n",
    "    scaling_comparison[f'{feature}_Original'] = df_processed[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Standardization (Z-score normalization)\n",
    "print(\"Standardization (Z-score normalization)\")\n",
    "standard_scaler = StandardScaler()\n",
    "df_standardized = df_processed.copy()\n",
    "df_standardized[numerical_features] = standard_scaler.fit_transform(df_processed[numerical_features])\n",
    "\n",
    "# Add standardized values to comparison DataFrame\n",
    "for feature in numerical_features:\n",
    "    scaling_comparison[f'{feature}_Standardized'] = df_standardized[feature]\n",
    "\n",
    "print(\"\\nFirst 5 rows after standardization:\")\n",
    "df_standardized[numerical_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Print standardization parameters\n",
    "print(\"Standardization parameters:\")\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    print(f\"{feature}: mean = {standard_scaler.mean_[i]:.4f}, std = {standard_scaler.scale_[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2. Min-Max Normalization\n",
    "print(\"Min-Max Normalization\")\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_normalized = df_processed.copy()\n",
    "df_normalized[numerical_features] = minmax_scaler.fit_transform(df_processed[numerical_features])\n",
    "\n",
    "# Add normalized values to comparison DataFrame\n",
    "for feature in numerical_features:\n",
    "    scaling_comparison[f'{feature}_MinMax'] = df_normalized[feature]\n",
    "\n",
    "print(\"\\nFirst 5 rows after min-max normalization:\")\n",
    "df_normalized[numerical_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Print min-max normalization parameters\n",
    "print(\"Min-Max normalization parameters:\")\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    print(f\"{feature}: min = {minmax_scaler.data_min_[i]:.4f}, max = {minmax_scaler.data_max_[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the distribution of original vs scaled features for Age\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(scaling_comparison['Age_Original'], kde=True)\n",
    "plt.title('Original Age')\n",
    "\n",
    "# Standardized distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(scaling_comparison['Age_Standardized'], kde=True)\n",
    "plt.title('Standardized Age')\n",
    "\n",
    "# Min-Max normalized distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(scaling_comparison['Age_MinMax'], kde=True)\n",
    "plt.title('Min-Max Normalized Age')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the distribution of original vs scaled features for Fare\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(scaling_comparison['Fare_Original'], kde=True)\n",
    "plt.title('Original Fare')\n",
    "\n",
    "# Standardized distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(scaling_comparison['Fare_Standardized'], kde=True)\n",
    "plt.title('Standardized Fare')\n",
    "\n",
    "# Min-Max normalized distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(scaling_comparison['Fare_MinMax'], kde=True)\n",
    "plt.title('Min-Max Normalized Fare')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing and Removing Outliers\n",
    "\n",
    "We'll use boxplots to visualize outliers and the IQR method to identify and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize outliers using boxplots\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.boxplot(x=feature, data=df_processed)\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify outliers using IQR method\n",
    "outliers_summary = {}\n",
    "for feature in numerical_features:\n",
    "    Q1 = df_processed[feature].quantile(0.25)\n",
    "    Q3 = df_processed[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df_processed[(df_processed[feature] < lower_bound) | (df_processed[feature] > upper_bound)]\n",
    "    outliers_count = len(outliers)\n",
    "    outliers_percent = (outliers_count / len(df_processed)) * 100\n",
    "    \n",
    "    outliers_summary[feature] = {\n",
    "        'count': outliers_count,\n",
    "        'percentage': outliers_percent,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound\n",
    "    }\n",
    "    \n",
    "    print(f\"{feature}: {outliers_count} outliers ({outliers_percent:.2f}% of data)\")\n",
    "    print(f\"  - Lower bound: {lower_bound:.2f}\")\n",
    "    print(f\"  - Upper bound: {upper_bound:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize outliers with scatter plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    \n",
    "    # Get bounds from summary\n",
    "    lower_bound = outliers_summary[feature]['lower_bound']\n",
    "    upper_bound = outliers_summary[feature]['upper_bound']\n",
    "    \n",
    "    # Create a boolean mask for outliers\n",
    "    is_outlier = (df_processed[feature] < lower_bound) | (df_processed[feature] > upper_bound)\n",
    "    \n",
    "    # Plot non-outliers and outliers with different colors\n",
    "    plt.scatter(range(len(df_processed)), df_processed[feature], c=is_outlier.map({True: 'red', False: 'blue'}), \n",
    "                alpha=0.5, label='Outlier' if True in is_outlier.values else 'No outliers')\n",
    "    \n",
    "    # Add horizontal lines for bounds\n",
    "    plt.axhline(y=lower_bound, color='green', linestyle='--', label=f'Lower bound: {lower_bound:.2f}')\n",
    "    plt.axhline(y=upper_bound, color='green', linestyle='--', label=f'Upper bound: {upper_bound:.2f}')\n",
    "    \n",
    "    plt.title(f'Outliers in {feature}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(feature)\n",
    "    plt.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Remove outliers\n",
    "# Create a copy of the dataframe before removing outliers\n",
    "df_no_outliers = df_processed.copy()\n",
    "total_rows_before = len(df_no_outliers)\n",
    "\n",
    "# Create a mask for rows to keep (not outliers in any feature)\n",
    "keep_mask = pd.Series(True, index=df_no_outliers.index)\n",
    "\n",
    "for feature in numerical_features:\n",
    "    lower_bound = outliers_summary[feature]['lower_bound']\n",
    "    upper_bound = outliers_summary[feature]['upper_bound']\n",
    "    feature_mask = (df_no_outliers[feature] >= lower_bound) & (df_no_outliers[feature] <= upper_bound)\n",
    "    keep_mask = keep_mask & feature_mask\n",
    "\n",
    "# Apply the mask to keep only non-outlier rows\n",
    "df_no_outliers = df_no_outliers[keep_mask]\n",
    "total_rows_after = len(df_no_outliers)\n",
    "rows_removed = total_rows_before - total_rows_after\n",
    "percent_removed = (rows_removed / total_rows_before) * 100\n",
    "\n",
    "print(f\"Total rows before outlier removal: {total_rows_before}\")\n",
    "print(f\"Total rows after outlier removal: {total_rows_after}\")\n",
    "print(f\"Rows removed: {rows_removed} ({percent_removed:.2f}% of data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare distributions before and after outlier removal for Age\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Before removal\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_processed['Age'], kde=True)\n",
    "plt.title('Age Before Outlier Removal')\n",
    "\n",
    "# After removal\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_no_outliers['Age'], kde=True)\n",
    "plt.title('Age After Outlier Removal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare distributions before and after outlier removal for Fare\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Before removal\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_processed['Fare'], kde=True)\n",
    "plt.title('Fare Before Outlier Removal')\n",
    "\n",
    "# After removal\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_no_outliers['Fare'], kde=True)\n",
    "plt.title('Fare After Outlier Removal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the cleaned dataset\n",
    "df_no_outliers.to_csv('titanic_cleaned.csv', index=False)\n",
    "print(\"Cleaned dataset saved to 'titanic_cleaned.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary of Preprocessing Steps\n",
    "\n",
    "In this notebook, we have performed the following preprocessing steps on the Titanic dataset:\n",
    "\n",
    "1. **Data Exploration**:\n",
    "   - Loaded the dataset and examined its structure\n",
    "   - Checked for missing values (none found)\n",
    "   - Visualized distributions of features\n",
    "\n",
    "2. **Missing Value Handling**:\n",
    "   - Demonstrated imputation techniques on artificially introduced missing values\n",
    "\n",
    "3. **Categorical Feature Encoding**:\n",
    "   - Applied label encoding to the 'Sex' column\n",
    "   - Applied one-hot encoding to the 'Sex' column\n",
    "\n",
    "4. **Feature Scaling**:\n",
    "   - Applied standardization (z-score normalization)\n",
    "   - Applied min-max normalization\n",
    "   - Compared the distributions before and after scaling\n",
    "\n",
    "5. **Outlier Detection and Removal**:\n",
    "   - Visualized outliers using boxplots\n",
    "   - Identified outliers using the IQR method\n",
    "   - Removed outliers and compared distributions\n",
    "\n",
    "6. **Saved the Cleaned Dataset**:\n",
    "   - Saved the fully preprocessed dataset for further analysis\n",
    "\n",
    "These preprocessing steps have prepared the data for machine learning modeling, ensuring that it is clean, properly formatted, and optimized for algorithm performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interview Questions and Answers\n",
    "\n",
    "Please refer to the separate markdown file `interview_questions.md` for detailed answers to the following interview questions:\n",
    "\n",
    "1. What are the different types of missing data?\n",
    "2. How do you handle categorical variables?\n",
    "3. What is the difference between normalization and standardization?\n",
    "4. How do you detect outliers?\n",
    "5. Why is preprocessing important in ML?\n",
    "6. What is one-hot encoding vs label encoding?\n",
    "7. How do you handle data imbalance?\n",
    "8. Can preprocessing affect model accuracy?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
